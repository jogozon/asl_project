# asl_project

George Danzelaud - danzelau@usc.edu
Joseph Gozon - gozon@usc.edu

For our CAIS++ Winter Project, George and I designed a computer vision multiclass classification algorithm to identify the 29 letter classes of American Sign Language in images. To train and test our model, we used a Kaggle dataset consisting of 87,000 images for training and 29 images for testing. The training set of images contains 3,000 photos for each of the 26 letters in ASL, as well as the signs for “delete,” “nothing,” and “space.” The test dataset contains a single photo for each letter and the three aforementioned signs. For preprocessing, we applied Random Resized Crop and Random Horizontal Flip on the training images in order to maximize the model’s effectiveness at recognizing ASL signs in a number of different orientations. At this point, we converted the training and testing images into tensors and normalized the data to eliminate redundancies and standardize our inputs into the model.

When constructing our model, we chose to implement transfer learning with ResNet 18. ResNet has a ReLU function which turns any negative inputs to 0 while simply returning non-negative inputs. We hoped this would help with the prediction due to the nonlinear nature of convolution. We also chose to use a Stochastic Gradient Descent optimizer with a learning rate of 1e-3 and a momentum of 0.9. A decayed rate of learning was employed by scheduling learning rates with a step size of 7 and a gamma of 0.1, which would aggressively cut the rate of learning by a factor of 10. This helps our model converge better and prevents overshooting a local minima. Additionally, the number of workers used was decreased from 4 to 0 as a lower number of workers increased the amount of time needed to train the model. Model training was decreased by more than 50%, from 25.6 minutes to 10.5 minutes. Cross entropy loss was used instead of mean squared error because the model’s main purpose was classification while MSE is an optimal method for linear regression.

We experimented with a range of epoch numbers from one to ten. The first epoch achieved a train loss score of 0.0477 and accuracy of 0.9936, with a test loss score of 0 and accuracy of 1.0. The fact that our model reached an accuracy of 100% after a single epoch led us to believe that it overfitted our data. That being said, the low validation loss suggested that this was not the case. Instead, we believe the similarity between the training and testing datasets justifies this phenomenon. Hence, for the assigned task of ASL letter recognition, our model performed remarkably well with our chosen architecture, procedures, and metrics.

This model could be used to implement an application to aid in teaching young children American Sign Language. An accuracy score of the child’s signs could be provided, along with the corresponding gestures to correct and increase the similarity to an ideal sign. So far, the main limitation would be the inability to provide real time feedback as the model was trained using pictures instead of video.  

Next steps for this project include supplementing the test data set with more images to more thoroughly assess the constructed model. With only 29 images to verify the accuracy and loss of the model, it is very likely that adding more images will decrease the resulting values due to the increased variety. Additionally, adding pictures of hands with a variety of skin tones and different backgrounds would be more applicable to the data the model may encounter if employed in realistic situations. 
